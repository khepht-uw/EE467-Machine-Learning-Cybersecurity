{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j_UUTq-PT1k"
   },
   "source": [
    "# EE 467 Lab 1: ML Pipeline for Spam Detection\n",
    "\n",
    "In this lab, we will go through the process of a typical machine learning task, and apply it to a cyber-security problem. We will build a binary classifier that detects spam emails. Like previous lab, we will leave out some code for you to complete. Refer to API references and search on Google for usage of libraries and functions. Refer to previous labs and search on Google for usage of libraries and functions, and ask TA or Instructor if you don't really have a clue.\n",
    "\n",
    "Before working on the code, we will need to install `NLTK` and `scikit-learn` for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1Zjx8guPT1m",
    "outputId": "f97ff580-1146-4711-e466-9e602ea9e7a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\kheph\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kheph\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kheph\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaEWTjp0PT1n"
   },
   "source": [
    "And ensure the dataset is extracted from the archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9is6J72DPT1n",
    "outputId": "3e74a2b2-74b9-4e49-9a31-ae39d5e565cf"
   },
   "outputs": [],
   "source": [
    "# Extract data\n",
    "!tar -xf emails.tar.xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRVyTUb3PT1n"
   },
   "source": [
    "Then import the libraries we will use here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lGr3vz7gPT1n",
    "outputId": "fb868e66-706b-4c9b-92f3-f1d34f919aba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kheph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "# string   - Python's built-in module for string operations (punctuation list)\n",
    "# numpy    - Numerical computing (we use 'np' as the standard alias)\n",
    "# pandas   - Data manipulation and analysis (we use 'pd' as the standard alias)\n",
    "# nltk     - Natural Language Toolkit for text processing\n",
    "# =============================================================================\n",
    "\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK (Natural Language Toolkit) - the most popular Python library for NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stop words (common words like \"the\", \"a\", \"is\" that add no meaning)\n",
    "# These need to be downloaded once before use\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elnpAo5yPT1o"
   },
   "source": [
    "## Pre-processing\n",
    "\n",
    "All machine learning tasks begin with the **pre-processing** step, during which we load the dataset into memory and \"clean\" the data so that they are suitable for subsequent steps. For spam email detection task, here we will load all emails into the memory, tokenize each email into a list of words and then remove words that are useless for analysis.\n",
    "\n",
    "All emails are stored in `emails.csv` under the same directory as this notebook. Feel free to open the file, take a look and get familiar with the format of the email dataset, then go back here to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qH2gqphfPT1o",
    "outputId": "27958fe5-c749-45a5-abd5-099951240dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  spam\n",
      "0  Subject: naturally irresistible your corporate...     1\n",
      "1  Subject: the stock trading gunslinger  fanny i...     1\n",
      "2  Subject: unbelievable new homes made easy  im ...     1\n",
      "3  Subject: 4 color printing special  request add...     1\n",
      "4  Subject: do not have money , get software cds ...     1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LOADING THE DATASET\n",
    "# =============================================================================\n",
    "# pd.read_csv() reads a CSV file and returns a DataFrame\n",
    "# A DataFrame is like a spreadsheet - rows are samples, columns are features\n",
    "# =============================================================================\n",
    "\n",
    "# Load email dataset into a DataFrame\n",
    "df = pd.read_csv(\"emails.csv\")\n",
    "\n",
    "# Preview first 5 rows\n",
    "print(df.head(5), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOZgxL_vPT1o",
    "outputId": "fc2e59d8-2554-4777-c1f4-2b2e7e2c331b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5728, 2)\n",
      "Columns: Index(['text', 'spam'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check dataset size and columns\n",
    "print(\"Shape:\", df.shape)      # (rows, columns)\n",
    "print(\"Columns:\", df.columns)  # 'text' = email, 'spam' = label (1=spam, 0=ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "FxaHyRLDPT1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape:  (5695, 2)\n"
     ]
    }
   ],
   "source": [
    "#DONE\n",
    "## [ TODO 1 ] Remove duplicate rows from the DataFrame\n",
    "#\n",
    "# Hint: DataFrames have a method for removing duplicates in-place.\n",
    "#       After removing, print the shape to verify - expect fewer rows.\n",
    "#       Look up: pandas DataFrame drop_duplicates documentation\n",
    "#\n",
    "df = df.drop_duplicates()\n",
    "print(\"New shape: \", df.shape)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "WqVsI-LKPT1p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "spam    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing (NAN, NaN, na) data for each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtoegYUOPT1p"
   },
   "source": [
    "After loading the email dataset into memory, we will need to remove punctuations and stop words from these emails. Stop words are common, useless words that should be ignored in analysis (such as a, an, the, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "xxXxXArePT1p"
   },
   "outputs": [],
   "source": [
    "# Text tokenizer: removes punctuation and stop words\n",
    "def process_text(text):\n",
    "    \"\"\"Convert email text to list of meaningful words.\"\"\"\n",
    "\n",
    "    # Remove punctuation (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "\n",
    "    # Remove stop words (\"the\", \"a\", \"is\", etc.) - case insensitive\n",
    "    clean_words = [word for word in nopunc.split()\n",
    "                   if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "EWBnGCsePT1p",
    "outputId": "7d05f7f0-5011-4bcb-94b3-ad1773771351"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Subject, naturally, irresistible, corporate, ...\n",
       "1    [Subject, stock, trading, gunslinger, fanny, m...\n",
       "2    [Subject, unbelievable, new, homes, made, easy...\n",
       "3    [Subject, 4, color, printing, special, request...\n",
       "4    [Subject, money, get, software, cds, software,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the result of tokenization\n",
    "df['text'].head().apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh4kfn-6PT1p"
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We have obtained semi-structured tokenized email texts in the pre-processing step; however, machine learning algorithms usually operate on fully-structured numerical features. Hence, we need to find a way to convert the email texts to numeric vectors. This process is called **feature extraction**, and is necessary in data mining and analysis tasks where input data is semi-structured or even unstructured. In the following part we will make use of `scikit-learn`, which is a library for classic machine learning and feature extraction.\n",
    "\n",
    "We will use **token count features** to represent the characteristics of each email. This turns a piece of text into a vector, each dimension of which contains the number of occurance of a particular word. In practice, we process many texts at once and end up getting a token count matrix. Below is simple demo on a toy dataset with only two emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "SR5lKkFAPT1p"
   },
   "outputs": [],
   "source": [
    "# DEMO: Bag-of-Words converts text → word count vectors\n",
    "\n",
    "message4 = 'hello world hello hello world play'\n",
    "message5 = 'test test test test one hello'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer: text → matrix where each column = a word, values = counts\n",
    "cv = CountVectorizer(analyzer=process_text)\n",
    "bow4 = cv.fit_transform([[message4], [message5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUj5D0SBPT1p",
    "outputId": "080b5163-3fae-4e3e-f7d8-3cddb7c8057d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello' 'one' 'play' 'test' 'world'] \n",
      "\n",
      "[[3 0 1 0 2]\n",
      " [1 1 0 4 0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary = unique words (these become column names)\n",
    "print(cv.get_feature_names_out(), \"\\n\")\n",
    "\n",
    "# Count matrix: rows = documents, columns = word counts\n",
    "print(bow4.toarray(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8IXHIprPT1p",
    "outputId": "a14d9c95-e1f4-4eb8-9513-4989a275b74b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 4)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t4\n",
      "  (1, 1)\t1 <class 'scipy.sparse._csr.csr_matrix'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sparse format: only stores non-zero values (saves memory)\n",
    "print(bow4, type(bow4), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgfOHsqUPT1p"
   },
   "source": [
    "Now let's compute and store token count matrix for real data:\n",
    "\n",
    "## Create bag-of-words matrix for all emails\n",
    "\n",
    "In this step, you will convert the email **text content** into a **Bag-of-Words (BoW)** representation using `CountVectorizer`.\n",
    "\n",
    "✅ **Important note:**  \n",
    "In the in-class demo, we used `CountVectorizer(analyzer=process_text)`, where `process_text` performs custom text processing.  \n",
    "That approach can be **slow** and may produce **many printed outputs** because the custom analyzer shows intermediate processing steps.\n",
    "\n",
    "For this lab, we will use a simpler and faster approach by letting `CountVectorizer` handle the tokenization internally, and we will enable English stop-word removal using:\n",
    "\n",
    "- `stop_words=\"english\"`\n",
    "\n",
    "➡️ Your task: apply `CountVectorizer(stop_words=\"english\")` on the `text` column and store the result in `messages_bow` as a **sparse matrix**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "9osgK5BsPT1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words without stop word removal: 37303\n",
      "Unique words with stop word removal: 36996\n"
     ]
    }
   ],
   "source": [
    "#DONE\n",
    "## [ TODO 2 ] Create bag-of-words matrix for all emails\n",
    "#\n",
    "# Hint: Use CountVectorizer with stop-word removal to fit and transform email text\n",
    "#       into a Bag-of-Words matrix.\n",
    "#       Apply it to the 'text' column of df. Store result in 'messages_bow'.\n",
    "#       Note: Keep it as sparse matrix (don't convert to array).\n",
    "#\n",
    "cv_no_stop = CountVectorizer()\n",
    "bow_no_stop = cv_no_stop.fit_transform(df['text'])\n",
    "vocab_no_stop = cv_no_stop.get_feature_names_out()\n",
    "print(\"Unique words without stop word removal:\", len(vocab_no_stop))\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "messages_bow = cv.fit_transform(df['text'])\n",
    "vocab = cv.get_feature_names_out()\n",
    "print(\"Unique words with stop word removal:\", len(vocab))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08AAR1-dPT1q"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have loaded and pre-processed the email dataset, it's time to **train** a classifier model that does the job. First, we will split the email dataset into a 80% **training set** and a 20% **test set**. Each set will contain sample features as well as corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "KWKaFDkbPT1q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into 80% training (X_train & y_train)\n",
    "# and 20% testing (X_test & y_test) data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages_bow, df['spam'], test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-PwjtkEPT1q"
   },
   "source": [
    "Then, we train a **logistic regression** classifier on the training set. We determine the class of the sample through its probability which is computed from the following formula:\n",
    "\n",
    "$$\n",
    "P(Y = 1|X = x) = \\frac{e^{\\mathbf{X}^T \\mathbf{b}}}{(1+e^{\\mathbf{X}^T \\mathbf{b}})} \\\\\n",
    "P(Y = 0|X = x) = 1 - P(Y = 1|X = x)\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{b}$ is a trainable vector. During training, we will try to maximize the **cross entropy loss** by performing **stochastic gradient descent** on parameter $\\mathbf{b}$:\n",
    "\n",
    "$$\n",
    "l_{CE} = -(y \\log P(Y = 1|X = x) + (1 - y) \\log P(Y = 0|X = x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "2qJIq3TYPT1q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#DONE\n",
    "## [ TODO 3 ] Create and train a logistic regression classifier\n",
    "#\n",
    "# Hint: Instantiate LogisticRegression (use random_state=0 for reproducibility).\n",
    "#       Then call the appropriate method to train on X_train and y_train.\n",
    "#       Store the model in a variable called 'classifier'.\n",
    "#\n",
    "classifier = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p45HPPxiPT1q"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally, we need to determine how good our classification model is. This is known as **evaluation**. We will use our trained model to make predictions for both training and testing data, and calculate various metrics with the predictions and actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "YrwgOf3CPT1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training prediction:\n",
      " [0 0 0 ... 0 0 0] \n",
      "\n",
      "Training actual:\n",
      " [0 0 0 ... 0 0 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print predictions on training data\n",
    "# `predict` function compute model predictions from input data\n",
    "print(\"Training prediction:\\n\", classifier.predict(X_train), \"\\n\")\n",
    "\n",
    "# Print the actual labels\n",
    "print(\"Training actual:\\n\", y_train.values, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03v6kf7KPT1q"
   },
   "source": [
    "There are a number of useful metrics for evaluation of binary classifiers, available through `classification_report`, `confusion_matrix` and `accuracy_score` functions:\n",
    "\n",
    "* **Confusion Matrix**: a matrix that indicates how many samples are correctly or incorrectly classified. The cell at $i$-th row and $j$-th column represents how many samples that belong to $i$-th class and are predicted as $j$-th class. For binary classification, the confusion matrix has only two columns and two rows:\n",
    "\n",
    "|Class|True               |False              |\n",
    "|-----|-------------------|-------------------|\n",
    "|True |True Positive (TP) |False Negative (FN)|\n",
    "|False|False Positive (FP)|True Negative (TN) |\n",
    "\n",
    "* **Accuracy**: proportion of samples that are correctly classified.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}\n",
    "$$\n",
    "\n",
    "* **Precision**: of all positive predictions, how many of them are actually correct?\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "* **Recall**: of all actually positive samples, how many of them are predicted correctly?\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "* **F1 Score**: the harmonic mean of precision and recall.\n",
    "\n",
    "$$\n",
    "F1 = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjGVNZnoPT1q"
   },
   "source": [
    "We first calculates and prints various metrics for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "_RRmxZqjPT1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3457\n",
      "           1       1.00      1.00      1.00      1099\n",
      "\n",
      "    accuracy                           1.00      4556\n",
      "   macro avg       1.00      1.00      1.00      4556\n",
      "weighted avg       1.00      1.00      1.00      4556\n",
      "\n",
      "Confusion Matrix: \n",
      " [[3457    0]\n",
      " [   0 1099]] \n",
      "\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Predict and evaluate on training data\n",
    "pred = classifier.predict(X_train)\n",
    "\n",
    "# `classification_report` outputs classification metrics\n",
    "# such as precision, recall and F1 score\n",
    "print(classification_report(y_train, pred))\n",
    "\n",
    "# `confusion_matrix` outputs how many samples are correctly or incorrectly classified\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_train, pred), \"\\n\")\n",
    "\n",
    "# `accuracy` computes classification accuracy\n",
    "print('Accuracy: ', accuracy_score(y_train, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os791MNHPT1q"
   },
   "source": [
    "We now calculate and print the same metrics for testing data. This measures the ability of the classification model to generalize to similar yet unknown data. The less difference in training and testing data, the better the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "df8bCez2PT1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Predicted  Actual\n",
      "977           1       1\n",
      "3275          0       0\n",
      "4163          0       0\n",
      "751           1       1\n",
      "3244          0       0\n",
      "5363          0       0\n",
      "4268          0       0\n",
      "1601          0       0\n",
      "5153          0       0\n",
      "4311          0       0\n"
     ]
    }
   ],
   "source": [
    "## [ TODO 4 ] Print test predictions and actual labels\n",
    "#\n",
    "# Hint: Use the trained classifier to predict on X_test.\n",
    "#       Print both the predictions and y_test values side by side.\n",
    "#       Follow the same pattern used for training data in Cell 25.\n",
    "#\n",
    "predict = classifier.predict(X_test)\n",
    "\n",
    "# Create a small DataFrame to see predictions vs actual labels\n",
    "results = pd.DataFrame({'Predicted': predict,'Actual': y_test})\n",
    "print(results[:10])\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "rK0oRo-jPT1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       870\n",
      "           1       0.98      0.98      0.98       269\n",
      "\n",
      "    accuracy                           0.99      1139\n",
      "   macro avg       0.99      0.99      0.99      1139\n",
      "weighted avg       0.99      0.99      0.99      1139\n",
      "\n",
      "Confusion Matrix: \n",
      " [[865   5]\n",
      " [  5 264]] \n",
      "\n",
      "Accuracy:  0.9912203687445127\n"
     ]
    }
   ],
   "source": [
    "## [ TODO 5 ] Evaluate classifier on test data\n",
    "#\n",
    "# Hint: Follow the same evaluation pattern used for training data previously.\n",
    "#       Use the three imported metrics functions on X_test/y_test.\n",
    "#       Expected accuracy should be around 98-99%.\n",
    "#\n",
    "print(classification_report(y_test, predict))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, predict), \"\\n\")\n",
    "print('Accuracy: ', accuracy_score(y_test, predict))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od8bz8ieTNui"
   },
   "source": [
    "## Discussion Question: Why Bag-of-Words (BoW) Still Works (and its Limitations)\n",
    "\n",
    "In this lab, we used **Bag-of-Words (BoW)** to convert email text into numerical features that a machine learning model can understand.\n",
    "\n",
    "### A common concern with BoW\n",
    "In the in-class discussion, we learned that some words appear in **many** documents (examples: *“the”*, *“and”*, *“hello”*, *“thanks”*). These very frequent words can cause two issues:\n",
    "\n",
    "1. **They do not help distinguish spam vs. ham**  \n",
    "   If a word appears in almost every email, it does not provide useful information for classification.\n",
    "\n",
    "2. **Different emails can look similar in feature space**  \n",
    "   Two different messages may share many common words, which can lead to **similar BoW representations**, even if their meaning is different.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Your Task (Short Answer)\n",
    "Even with the limitations above, BoW often performs surprisingly well for spam detection.\n",
    "\n",
    "**Why does the Bag-of-Words method still work well in this lab?**  \n",
    "Write a **short explanation** (2–4 sentences) and include **at least one clear reason** supported by what you observe in the dataset or model behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "7Q7rbT8NWCB-"
   },
   "outputs": [],
   "source": [
    "#The Bag-of-Words method is still effective in this lab because we included the stop_words = \"english\" in our count vectorization.\n",
    "#This adjustment removes all the common english stop words that aren't conducive towards text analysis, like\n",
    "#\"the\", \"is\", \"and\", etc. This removes the extremely common factors between the different email bodys, making it less likely\n",
    "#for different BoW representations to look similar. In the dataset, we can see that the unique word count drops from 37303\n",
    "#to 36996 after applying the word stop, which supports my analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zltAtnu8PT1q"
   },
   "source": [
    "## References\n",
    "1. https://github.com/randerson112358/Python/blob/master/Email_Spam_Detection/Email_Spam_Detection.ipynb\n",
    "2. https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
